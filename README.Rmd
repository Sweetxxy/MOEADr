---
title: "MOEADr Experiments"
author: Felipe Campelo, Lucas Batista, Claus Aranha
output: html_document
---

This file describes how to reproduce the experiments using the MOEADr package, as 
part of the results section of the manuscript "_A Component-Wise Perspective on 
Multiobjective Evolutionary Algorithms based on Decomposition_". 

***
# Experiment 1: MOEADr + irace

## Introduction
This section contains the experimental setup for the MOEA/D tuning experiments using the MOEADr framework. In this experiment we employ the standard implementation of the [Iterated Racing](http://iridia.ulb.ac.be/irace/), available in the R package [irace](https://cran.r-project.org/web/packages/irace/index.html) (version 2.1).

The reference Pareto-optimal points used for the IGD calculations were obtained from [Qingfu Zhang's home page](http://dces.essex.ac.uk/staff/qzhang/moeacompetition09.htm), in the *pf_data* folder within [this file](http://dces.essex.ac.uk/staff/qzhang/MOEAcompetition/testproblemsourcecode0904.rar).

## How to reproduce the experimental results
To reproduce the results of the manuscript, follow the instructions below:

1. First download the _Manuscript-Version_ branch of the [MOEADr repository](https://github.com/fcampelo/MOEADr/tree/Manuscript-Version) ([click here](https://github.com/fcampelo/MOEADr/archive/Manuscript-Version.zip) for the least-effort way), and unzip it, if needed, in a working folder at your local machine.  
2. Make sure that the manuscript version of the package is installed (and, if not, install the correct version):

```{r, eval = FALSE}
if (!("MOEADr" %in% rownames(installed.packages())) | (packageVersion("MOEADr") != "0.2.0")) {
  require(devtools)
  devtools::install_github("fcampelo/MOEADr/MOEADr@Manuscript-Version")
}
```

3. Set the working directory of the analysis as the folder where you saved the files.

4. Run the script *run_irace_experiment.R*:

```{r, eval = FALSE}
source("run_irace_experiment.R")
```

## Training instances
In this experiment we adopted the general idea suggested by Bezerra et al. [1], using functions UF1 - UF10 from the the [CEC'2009 benchmark set](http://dces.essex.ac.uk/staff/qzhang/moeacompetition09.htm) and employing disjoint sets of problem dimensions for tuning and evaluation, with $n_v^{tune} \in \left\{20,\dotsc,60\right\} \backslash n_v^{test}$ and $n_v^{test} \in \left\{30,40,50\right\}$. The standard implementations of these functions available in the R package [smoof](https://cran.r-project.org/web/packages/smoof/index.html) (version 1.4) were used.

Following the [definitions of the CEC'2009 competition](http://dces.essex.ac.uk/staff/qzhang/moeacompetition09.htm), the number of subproblems used was fixed as 100 for $m = 2$, and 150 for $m = 3$ (for the Simplex Lattice Design the actual number was 153 for $m = 3$, given the specifics of this method. Please refer to the manuscript for details).


## Further comments regarding the design of this experiment
This tuning experiment was intended at exploring the potential of the MOEADr framework in leveraging the capabilities of automated algorithm assembly / tuning offered by the Iterated Racing method. Some design decisions had to be made in the design of this experiment, as detailed below (for a complete view of the search space for tuning, see file _./Experiments/irace tuning/parameters.txt_)

- constraint handling: *none* (since the problems are unconstrained)
- Variation stack: we opted for defining a variation stack with between 3 and 4 operators, using the following rationale:  
- first and second operators chosen from the existing "traditional" variation operators available in the MOEADr package: *SBX*, *Polynomial mutation*, *Differential mutation*, and *Binomial Recombination*  
- third operator: can be any of the operators listed above, or *none* (in which case a third operator is not included)  
- fourth operator: can be either *Local search* or *none*.
- after the four operators have been executed, the variation stack wraps up by truncating solutions to the variable limits. This is done simply to prevent problems with some test functions that return NaN's for points outside their domains.
- specific parameters of each operator are tuned independently for each position (e.g., if a configuration with two sequential SBX operators happens, each one has its own independent set of parameters)  
- local search done probabilistically (using $\gamma_{ls}\in(0,0.5)$) when local search is present.  
- Aggregation functions: WT, AWT, and PBI. *Inverted PBI* was not included as a possible scalarization function, since we couldn't validate the MOEADr implementation against the original code (our attempts to reach the author remain unanswered)
- Decomposition strategies: SLD and Uniform. *MSLD* was not included as a possible decomposition method, since we are dealing only with 2 and 3 objectives, which makes it generally unnecessary.


## References

- [1] L. Bezerra, M. Lopez-Ibanez, T. Stutzle, "Automatic Component-wise Design of Multi-objective Evolutionary Algorithms". DOI: [10.1109/TEVC.2015.2474158](http://dx.doi.org/10.1109/TEVC.2015.2474158).  
